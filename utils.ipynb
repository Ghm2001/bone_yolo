{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb79d26",
   "metadata": {},
   "source": [
    "æå–å•ä¸ªniiæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8a6db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æå–å•ä¸ªniiæ–‡ä»¶\n",
    "import os\n",
    "import SimpleITK as sitk\n",
    "import pydicom\n",
    "import re\n",
    "\n",
    "def is_valid_dicom(file_path):\n",
    "    \"\"\"æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºæœ‰æ•ˆçš„DICOMæ–‡ä»¶\"\"\"\n",
    "    try:\n",
    "        pydicom.dcmread(file_path, stop_before_pixels=True)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def get_dicom_series_desc(file_path):\n",
    "    \"\"\"ä»…è·å–DICOMæ–‡ä»¶çš„åºåˆ—æè¿°ï¼ˆæ— éœ€æ‚£è€…åï¼ŒæŒ‰åŸå§‹æ–‡ä»¶åå‘½åï¼‰\"\"\"\n",
    "    try:\n",
    "        ds = pydicom.dcmread(file_path, stop_before_pixels=True)\n",
    "        return ds.SeriesDescription if hasattr(ds, 'SeriesDescription') else \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ è¯»å–DICOMåºåˆ—æè¿°å¤±è´¥ {os.path.basename(file_path)}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def get_existing_files(output_dir):\n",
    "    \"\"\"æ‰«æè¾“å‡ºç›®å½•ï¼Œè·å–å·²å¤„ç†çš„æ–‡ä»¶ï¼ˆåŸºäºåŸå§‹æ–‡ä»¶å_front.nii.gzï¼‰\"\"\"\n",
    "    existing_original_names = set()\n",
    "    if not os.path.exists(output_dir):\n",
    "        return existing_original_names\n",
    "    \n",
    "    # éå†è¾“å‡ºç›®å½•ä¸­æ‰€æœ‰_front.nii.gzæ–‡ä»¶\n",
    "    for filename in os.listdir(output_dir):\n",
    "        if filename.endswith(\"_front.nii.gz\"):\n",
    "            # æå–åŸå§‹æ–‡ä»¶åï¼ˆå»æ‰ \"_front.nii.gz\" åç¼€ï¼‰\n",
    "            original_name = filename.replace(\"_front.nii.gz\", \"\")\n",
    "            existing_original_names.add(original_name)\n",
    "    \n",
    "    print(f\"â„¹ï¸ å·²å‘ç° {len(existing_original_names)} ä¸ªå·²å¤„ç†æ–‡ä»¶ï¼ˆåŸºäºåŸå§‹æ–‡ä»¶åï¼‰\")\n",
    "    return existing_original_names\n",
    "\n",
    "def export_wb_series(input_dir, output_root, wb_keyword=\"WB\"):\n",
    "    \"\"\"é€‚é…å•ä¸ªDICOMæ–‡ä»¶åœºæ™¯ï¼ŒæŒ‰ã€ŒåŸå§‹æ–‡ä»¶å_front.nii.gzã€å¯¼å‡ºï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ \"\"\"\n",
    "    # æ£€æŸ¥è¾“å…¥è·¯å¾„\n",
    "    if not os.path.exists(input_dir):\n",
    "        print(f\"é”™è¯¯ï¼šè¾“å…¥è·¯å¾„ä¸å­˜åœ¨ - {input_dir}\")\n",
    "        return\n",
    "    if not os.path.isdir(input_dir):\n",
    "        print(f\"é”™è¯¯ï¼šè¾“å…¥è·¯å¾„ä¸æ˜¯æ–‡ä»¶å¤¹ - {input_dir}\")\n",
    "        return\n",
    "        \n",
    "    os.makedirs(output_root, exist_ok=True)\n",
    "    print(f\"è¾“å‡ºæ–‡ä»¶å°†ä¿å­˜åˆ°ï¼š{output_root}\")\n",
    "    print(f\"æ–‡ä»¶å‘½åæ ¼å¼ï¼šåŸå§‹æ–‡ä»¶å_front.nii.gz\")\n",
    "    print(f\"æ­£åœ¨æ‰«ææ–‡ä»¶å¤¹ä¸­çš„å•ä¸ªDICOMæ–‡ä»¶ï¼š{input_dir}\\n\")\n",
    "    \n",
    "    # å…³é”®ä¿®æ”¹1ï¼šè·å–å·²å¤„ç†çš„åŸå§‹æ–‡ä»¶åï¼ˆæ–­ç‚¹ç»­ä¼ æ ¸å¿ƒï¼‰\n",
    "    processed_original_names = get_existing_files(output_root)\n",
    "    valid_dicom_count = 0       # æœ‰æ•ˆDICOMæ–‡ä»¶è®¡æ•°\n",
    "    newly_processed = 0         # æœ¬æ¬¡æ–°å¢å¤„ç†çš„æ•°é‡\n",
    "    \n",
    "    # éå†æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰æ–‡ä»¶\n",
    "    for filename in os.listdir(input_dir):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        # è·³è¿‡æ–‡ä»¶å¤¹ï¼Œåªå¤„ç†æ–‡ä»¶\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "        \n",
    "        # è·³è¿‡éDICOMæ–‡ä»¶ï¼ˆä»…ä¿ç•™.dcmåç¼€ï¼‰\n",
    "        if not filename.lower().endswith('.dcm'):\n",
    "            continue\n",
    "        \n",
    "        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆDICOM\n",
    "        if not is_valid_dicom(file_path):\n",
    "            print(f\"â„¹ï¸ è·³è¿‡æ— æ•ˆDICOMæ–‡ä»¶ï¼š{filename}\")\n",
    "            continue\n",
    "        \n",
    "        valid_dicom_count += 1\n",
    "        series_desc = get_dicom_series_desc(file_path)\n",
    "        \n",
    "        # å…³é”®ä¿®æ”¹2ï¼šæå–åŸå§‹æ–‡ä»¶åï¼ˆä¸å«.dcmåç¼€ï¼‰ï¼Œå¤„ç†ç‰¹æ®Šå­—ç¬¦ç¡®ä¿ä¿å­˜åˆæ³•\n",
    "        original_name_without_ext = os.path.splitext(filename)[0]\n",
    "        # æ›¿æ¢Windowséæ³•å­—ç¬¦ï¼ˆ\\ / : * ? \" < > |ï¼‰ä¸ºä¸‹åˆ’çº¿ï¼Œé¿å…ä¿å­˜å¤±è´¥\n",
    "        safe_original_name = re.sub(r'[\\\\/:*?\"<>|]', '_', original_name_without_ext)\n",
    "        \n",
    "        # å…³é”®ä¿®æ”¹3ï¼šåŸºäºåŸå§‹æ–‡ä»¶ååˆ¤æ–­æ˜¯å¦å·²å¤„ç†ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰\n",
    "        if safe_original_name in processed_original_names:\n",
    "            print(f\"â„¹ï¸ æ–‡ä»¶å·²å¤„ç†ï¼Œè·³è¿‡ï¼š{filename} â†’ å¯¹åº”è¾“å‡ºï¼š{safe_original_name}_front.nii.gz\")\n",
    "            continue\n",
    "        \n",
    "        # ç­›é€‰WBåºåˆ—ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰\n",
    "        if wb_keyword.upper() not in (series_desc or \"\").upper():\n",
    "            print(f\"â„¹ï¸ è·³è¿‡éWBåºåˆ— - æ–‡ä»¶åï¼š{filename} | åºåˆ—æè¿°ï¼š{series_desc}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # è¯»å–å•ä¸ªDICOMæ–‡ä»¶\n",
    "            image = sitk.ReadImage(file_path)\n",
    "            \n",
    "            # å…³é”®ä¿®æ”¹4ï¼šæŒ‰ã€ŒåŸå§‹æ–‡ä»¶å_front.nii.gzã€ä¿å­˜\n",
    "            output_filename = os.path.join(output_root, f\"{safe_original_name}_front.nii.gz\")\n",
    "            \n",
    "            # ä¿å­˜ä¸ºnii.gz\n",
    "            sitk.WriteImage(image, output_filename)\n",
    "            print(f\"âœ… å·²å¯¼å‡º - åŸå§‹æ–‡ä»¶ï¼š{filename} | WBåºåˆ—ï¼š{series_desc} â†’ è¾“å‡ºï¼š{os.path.basename(output_filename)}\")\n",
    "            \n",
    "            # æ ‡è®°ä¸ºå·²å¤„ç†ï¼ˆé¿å…æœ¬æ¬¡è¿è¡Œé‡å¤å¤„ç†ï¼‰\n",
    "            processed_original_names.add(safe_original_name)\n",
    "            newly_processed += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ å¤„ç†å¤±è´¥ - åŸå§‹æ–‡ä»¶ï¼š{filename} | é”™è¯¯ï¼š{str(e)}\")\n",
    "    \n",
    "    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"æ‰«æå®Œæˆï¼šå…±å‘ç° {valid_dicom_count} ä¸ªæœ‰æ•ˆDICOMæ–‡ä»¶\")\n",
    "    print(f\"å·²å¤„ç†ï¼ˆå†å²+æœ¬æ¬¡ï¼‰ï¼š{len(processed_original_names)} ä¸ªWBåºåˆ—æ–‡ä»¶\")\n",
    "    print(f\"æœ¬æ¬¡æ–°å¢å¤„ç†ï¼š{newly_processed} ä¸ªWBåºåˆ—æ–‡ä»¶\")\n",
    "    print(f\"è¾“å‡ºè·¯å¾„ï¼š{output_root}\")\n",
    "    print(f\"æ–‡ä»¶å‘½åæ ¼å¼ï¼šåŸå§‹æ–‡ä»¶å_front.nii.gz\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # é…ç½®è·¯å¾„ï¼ˆæ‰€æœ‰å•ä¸ªDICOMæ–‡ä»¶æ‰€åœ¨çš„æ–‡ä»¶å¤¹ï¼‰\n",
    "    INPUT_DIRECTORY = r\"/home/wangnannan/data/spect/sz/all_images\"\n",
    "    OUTPUT_DIRECTORY = r\"/home/gonghanmei/datasets/shen_data/only_wb_all_x\"\n",
    "    WB_KEYWORD = \"WB\"  # å¯è°ƒæ•´ä¸ºå®é™…WBåºåˆ—æè¿°ï¼ˆå¦‚\"Whole Body\"ã€\"å…¨èº«\"ç­‰ï¼‰\n",
    "    \n",
    "    export_wb_series(INPUT_DIRECTORY, OUTPUT_DIRECTORY, WB_KEYWORD)\n",
    "    print(\"\\næ‰¹é‡å¤„ç†å®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2531e211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5å•çº¯å¤„ç†nii to png\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "def nii_to_png(nii_file_path, output_dir, mirror=True):\n",
    "    # è¯»å–niiæ–‡ä»¶\n",
    "    img = nib.load(nii_file_path)\n",
    "    data = img.get_fdata()\n",
    "    \n",
    "    # è·å–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰\n",
    "    file_name = os.path.splitext(os.path.splitext(os.path.basename(nii_file_path))[0])[0]\n",
    "    \n",
    "    # ä¿®æ­£ï¼šbackå’Œfrontå¯¹è°ƒï¼Œå¹¶æ·»åŠ é¡ºæ—¶é’ˆæ—‹è½¬90Â°\n",
    "    # æ­£é¢åˆ‡ç‰‡ä¿å­˜ï¼ˆåŸåé¢ï¼‰\n",
    "    front_slice = data[:, :, 0]  # åŸåé¢åˆ‡ç‰‡\n",
    "    # é¡ºæ—¶é’ˆæ—‹è½¬90åº¦ï¼ˆä½¿ç”¨-90è¡¨ç¤ºé¡ºæ—¶é’ˆï¼‰\n",
    "    front_rotated = rotate(front_slice, -90, reshape=True)\n",
    "    \n",
    "    # å¦‚æœéœ€è¦é•œåƒå¤„ç†ï¼Œè¿›è¡Œå·¦å³ç¿»è½¬\n",
    "    if mirror:\n",
    "        front_rotated = np.fliplr(front_rotated)\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(front_rotated, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    front_output_path = os.path.join(output_dir, f\"{file_name}_front.png\")\n",
    "    plt.savefig(front_output_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    \n",
    "    # åé¢åˆ‡ç‰‡ä¿å­˜ï¼ˆåŸæ­£é¢ï¼‰\n",
    "    back_slice = data[:, :, 1]  # åŸæ­£é¢åˆ‡ç‰‡\n",
    "    # é¡ºæ—¶é’ˆæ—‹è½¬90åº¦\n",
    "    back_rotated = rotate(back_slice, -90, reshape=True)\n",
    "    \n",
    "    # å¦‚æœéœ€è¦é•œåƒå¤„ç†ï¼Œè¿›è¡Œå·¦å³ç¿»è½¬\n",
    "    if mirror:\n",
    "        back_rotated = np.fliplr(back_rotated)\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(back_rotated, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    back_output_path = os.path.join(output_dir, f\"{file_name}_back.png\")\n",
    "    plt.savefig(back_output_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "def batch_convert(nii_dir, output_dir, mirror=True):\n",
    "    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # éå†ç›®å½•ä¸‹çš„æ‰€æœ‰nii.gzæ–‡ä»¶\n",
    "    for file in os.listdir(nii_dir):\n",
    "        if file.endswith('.nii.gz'):\n",
    "            nii_file_path = os.path.join(nii_dir, file)\n",
    "            nii_to_png(nii_file_path, output_dir, mirror)\n",
    "            print(f\"å·²å¤„ç†ï¼š{file}\")\n",
    "\n",
    "\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "if __name__ == \"__main__\":\n",
    "    # æ›¿æ¢ä¸ºä½ çš„niiæ–‡ä»¶æ‰€åœ¨æ–‡ä»¶å¤¹è·¯å¾„\n",
    "    nii_directory = r\"/home/wangnannan/data/spect/sz/all_images_denoise\"\n",
    "    # æ›¿æ¢ä¸ºä½ å¸Œæœ›ä¿å­˜pngå›¾ç‰‡çš„æ–‡ä»¶å¤¹è·¯å¾„\n",
    "    output_directory = r\"/home/gonghanmei/datasets/shen_data/all_data/png-all-qz\"\n",
    "    \n",
    "    # å¯ä»¥é€šè¿‡è®¾ç½®mirrorå‚æ•°æ§åˆ¶æ˜¯å¦é•œåƒï¼Œé»˜è®¤ä¸ºTrue\n",
    "    batch_convert(nii_directory, output_directory, mirror=True)\n",
    "    print(\"æ‰¹é‡è½¬æ¢å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399eed8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1æå–æ‰€æœ‰csvæ–‡ä»¶\n",
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "def merge_csv_files():\n",
    "    # è·å–bone-dataæ–‡ä»¶å¤¹çš„è·¯å¾„\n",
    "    bone_data_path = r\"/home/DataCollection/Raw/SPECT/SZ_Cancer/W8-bone 62\"\n",
    "    \n",
    "    # æ£€æŸ¥bone-dataæ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨\n",
    "    if not os.path.exists(bone_data_path):\n",
    "        print(f\"é”™è¯¯ï¼šæœªæ‰¾åˆ°æ–‡ä»¶å¤¹ {bone_data_path}\")\n",
    "        return\n",
    "    \n",
    "    # è·å–bone-dataçš„ä¸Šçº§ç›®å½•\n",
    "    parent_dir = os.path.dirname(bone_data_path)\n",
    "    \n",
    "    # æ–°å»ºä¿å­˜åˆå¹¶ç»“æœçš„æ–‡ä»¶å¤¹ï¼ˆåœ¨bone-dataåŒçº§ï¼‰\n",
    "    output_dir = os.path.join(parent_dir, \"csv\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"åˆå¹¶åçš„æ–‡ä»¶å°†ä¿å­˜åˆ°ï¼š{output_dir}\")\n",
    "    \n",
    "    # éå†bone-dataä¸‹çš„æ‰€æœ‰ç›´æ¥å­ç›®å½•\n",
    "    for root, dirs, files in os.walk(bone_data_path):\n",
    "        if root == bone_data_path:  # åªå¤„ç†ç›´æ¥å­ç›®å½•\n",
    "            for subdir in dirs:\n",
    "                subdir_path = os.path.join(root, subdir)\n",
    "                print(f\"\\næ­£åœ¨å¤„ç†ç›®å½•ï¼š{subdir_path}\")\n",
    "                \n",
    "                # æŸ¥æ‰¾è¯¥ç›®å½•ä¸‹æ‰€æœ‰CSVæ–‡ä»¶ï¼ˆæŒ‰æ–‡ä»¶åæ’åºï¼Œç¡®ä¿é¡ºåºä¸€è‡´ï¼‰\n",
    "                csv_files = sorted(glob(os.path.join(subdir_path, \"*.csv\")))\n",
    "                \n",
    "                # å‡†å¤‡è¾“å‡ºæ–‡ä»¶è·¯å¾„\n",
    "                output_file = os.path.join(output_dir, f\"{subdir}.csv\")\n",
    "                \n",
    "                if not csv_files:\n",
    "                    print(f\"  è­¦å‘Šï¼šè¯¥ç›®å½•ä¸‹æœªæ‰¾åˆ°CSVæ–‡ä»¶ï¼Œå°†ç”Ÿæˆç©ºæ–‡ä»¶\")\n",
    "                    pd.DataFrame().to_csv(output_file, index=False)\n",
    "                    print(f\"  å·²ç”Ÿæˆç©ºæ–‡ä»¶ï¼š{os.path.basename(output_file)}\")\n",
    "                    continue\n",
    "                \n",
    "                # åˆå¹¶æ‰€æœ‰CSVæ–‡ä»¶ï¼Œä¿æŒç¬¬ä¸€åˆ—åˆ†ç»„é€’å¢\n",
    "                dfs = []\n",
    "                last_group = 0  # è®°å½•ä¸Šä¸€ä¸ªæ–‡ä»¶çš„æœ€å¤§åˆ†ç»„å·\n",
    "                first_col_name = None  # ç¬¬ä¸€åˆ—çš„åˆ—å\n",
    "                \n",
    "                for i, csv_file in enumerate(csv_files):\n",
    "                    try:\n",
    "                        df = pd.read_csv(csv_file)\n",
    "                        if df.empty:\n",
    "                            print(f\"  è­¦å‘Šï¼š{os.path.basename(csv_file)} ä¸ºç©ºæ–‡ä»¶ï¼Œè·³è¿‡\")\n",
    "                            continue\n",
    "                        \n",
    "                        # è·å–ç¬¬ä¸€åˆ—åˆ—åï¼ˆé¦–æ¬¡è¯»å–æ—¶ç¡®å®šï¼‰\n",
    "                        if first_col_name is None:\n",
    "                            first_col_name = df.columns[0]\n",
    "                            print(f\"  æ£€æµ‹åˆ°ç¬¬ä¸€åˆ—åˆ—åä¸ºï¼š{first_col_name}ï¼ˆå°†ä¿æŒè¯¥åˆ—åˆ†ç»„é€’å¢ï¼‰\")\n",
    "                        \n",
    "                        # æ£€æŸ¥å½“å‰æ–‡ä»¶æ˜¯å¦æœ‰ç¬¬ä¸€åˆ—\n",
    "                        if first_col_name not in df.columns:\n",
    "                            print(f\"  è­¦å‘Šï¼š{os.path.basename(csv_file)} ç¼ºå°‘ç¬¬ä¸€åˆ— {first_col_name}ï¼Œè·³è¿‡\")\n",
    "                            continue\n",
    "                        \n",
    "                        # æå–å½“å‰æ–‡ä»¶çš„ç¬¬ä¸€åˆ—æ•°æ®\n",
    "                        current_group_col = df[first_col_name]\n",
    "                        \n",
    "                        # å¤„ç†ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼šä»¥å…¶è‡ªèº«çš„åˆ†ç»„ä¸ºåŸºå‡†\n",
    "                        if i == 0:\n",
    "                            # ç¡®ä¿ç¬¬ä¸€åˆ—æ˜¯æ•°å€¼ç±»å‹\n",
    "                            if not pd.api.types.is_numeric_dtype(current_group_col):\n",
    "                                print(f\"  è­¦å‘Šï¼š{os.path.basename(csv_file)} çš„ç¬¬ä¸€åˆ—ä¸æ˜¯æ•°å€¼ç±»å‹ï¼Œæ— æ³•å¤„ç†ï¼Œè·³è¿‡\")\n",
    "                                continue\n",
    "                            # æ›´æ–°æœ€åä¸€ä¸ªåˆ†ç»„å·\n",
    "                            last_group = current_group_col.max()\n",
    "                            dfs.append(df)\n",
    "                            print(f\"  å·²è¯»å–ï¼š{os.path.basename(csv_file)}ï¼ˆæœ€å¤§åˆ†ç»„ï¼š{last_group}ï¼‰\")\n",
    "                        else:\n",
    "                            # éç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼šå°†å…¶åˆ†ç»„å·æ•´ä½“åç§»ï¼Œç¡®ä¿ä» last_group + 1 å¼€å§‹\n",
    "                            # è®¡ç®—å½“å‰æ–‡ä»¶çš„åˆ†ç»„åç§»é‡ï¼ˆå½“å‰æœ€å°åˆ†ç»„åº”è¯¥å¯¹åº” last_group + 1ï¼‰\n",
    "                            current_min = current_group_col.min()\n",
    "                            offset = (last_group + 1) - current_min\n",
    "                            \n",
    "                            # åç§»å½“å‰æ–‡ä»¶çš„ç¬¬ä¸€åˆ—ï¼ˆä¿è¯åˆ†ç»„è¿ç»­é€’å¢ï¼‰\n",
    "                            df[first_col_name] = current_group_col + offset\n",
    "                            \n",
    "                            # æ›´æ–°æœ€åä¸€ä¸ªåˆ†ç»„å·\n",
    "                            last_group = df[first_col_name].max()\n",
    "                            dfs.append(df)\n",
    "                            print(f\"  å·²è¯»å–ï¼š{os.path.basename(csv_file)}ï¼ˆåç§»åæœ€å¤§åˆ†ç»„ï¼š{last_group}ï¼‰\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"  è¯»å–å¤±è´¥ {os.path.basename(csv_file)}ï¼š{str(e)}\")\n",
    "                \n",
    "                if not dfs:\n",
    "                    print(f\"  è­¦å‘Šï¼šè¯¥ç›®å½•ä¸‹æ‰€æœ‰CSVæ–‡ä»¶è¯»å–å¤±è´¥ï¼Œå°†ç”Ÿæˆç©ºæ–‡ä»¶\")\n",
    "                    pd.DataFrame().to_csv(output_file, index=False)\n",
    "                    print(f\"  å·²ç”Ÿæˆç©ºæ–‡ä»¶ï¼š{os.path.basename(output_file)}\")\n",
    "                    continue\n",
    "                \n",
    "                # åˆå¹¶æ‰€æœ‰æ•°æ®æ¡†ï¼ˆä¿æŒåŸæœ‰é¡ºåºï¼‰\n",
    "                merged_df = pd.concat(dfs, ignore_index=True)\n",
    "                \n",
    "                # ä¿å­˜åˆå¹¶åçš„æ–‡ä»¶\n",
    "                merged_df.to_csv(output_file, index=False)\n",
    "                print(f\"  åˆå¹¶å®Œæˆï¼Œå·²ä¿å­˜åˆ°ï¼š{os.path.basename(output_file)}ï¼ˆæœ€ç»ˆæœ€å¤§åˆ†ç»„ï¼š{last_group}ï¼‰\")\n",
    "    \n",
    "    print(\"\\næ‰€æœ‰ç›®å½•å¤„ç†å®Œæ¯•ï¼\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merge_csv_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8a8c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3è½¬åŒ–ä¸ºtxt\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def get_label(df, name: str, output_dir: str = None):\n",
    "    \"\"\"\n",
    "    å°†CSVæ•°æ®æŒ‰ç…§axis-0çš„å€¼æ‹†åˆ†ä¸ºfront(0)å’Œback(1)ä¸¤ä¸ªtxtæ–‡ä»¶\n",
    "    å³ä½¿æ²¡æœ‰æ•°æ®ä¹Ÿä¼šç”Ÿæˆç©ºæ–‡ä»¶\n",
    "    \"\"\"\n",
    "    # è®¾ç½®è¾“å‡ºç›®å½•\n",
    "    labels_path = Path(output_dir) if output_dir else Path('.')\n",
    "    labels_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    filter_column = 'axis-0'\n",
    "    value_for_front = 0\n",
    "    value_for_back = 1\n",
    "    \n",
    "    # å¤„ç† front (axis-0 = 0)\n",
    "    print(f\"å¤„ç† front æ•°æ® (axis-0 = {value_for_front})...\")\n",
    "    result = []\n",
    "    \n",
    "    # å¤„ç†ç©ºDataFrameæƒ…å†µ\n",
    "    if not df.empty and filter_column in df.columns:\n",
    "        front_df = df[df[filter_column] == value_for_front]\n",
    "        front_list = list(zip(front_df.get('axis-2', []).tolist(), \n",
    "                             front_df.get('axis-1', []).tolist()))[::2]\n",
    "        \n",
    "        # æŒ‰å¯¹å¤„ç†åæ ‡\n",
    "        for i in range(0, len(front_list), 2):\n",
    "            if i + 1 < len(front_list):\n",
    "                coord1 = front_list[i]\n",
    "                coord2 = front_list[i + 1]\n",
    "                \n",
    "                x_center = (coord1[0] + coord2[0]) / 2 / 256\n",
    "                y_center = (coord1[1] + coord2[1]) / 2 / 1024\n",
    "                width = (coord2[0] - coord1[0] + 40) / 2 / 256\n",
    "                height = (coord2[1] - coord1[1] + 40) / 2 / 1024\n",
    "                \n",
    "                result.append((0, x_center, y_center, width, height))\n",
    "    \n",
    "    # å†™å…¥frontæ–‡ä»¶ï¼ˆç¡®ä¿ç”Ÿæˆï¼‰\n",
    "    front_file_name = labels_path / (name + '_front.txt')\n",
    "    print(f\"ğŸ“ æ­£åœ¨å†™å…¥frontæ–‡ä»¶: {front_file_name}\")\n",
    "    with open(front_file_name, 'w', encoding='utf-8') as f:\n",
    "        for sublist in result:\n",
    "            line = ' '.join(map(str, sublist))\n",
    "            f.write(line + '\\n')\n",
    "    \n",
    "    print(f\"âœ… Frontæ–‡ä»¶å·²ä¿å­˜: {front_file_name}\")\n",
    "    print(f\"   æ–‡ä»¶å¤§å°: {os.path.getsize(front_file_name)} å­—èŠ‚\")\n",
    "    print(f\"   åŒ…å« {len(result)} ä¸ªæ ‡ç­¾\")\n",
    "    \n",
    "    # å¤„ç† back (axis-0 = 1)\n",
    "    print(f\"\\nå¤„ç† back æ•°æ® (axis-0 = {value_for_back})...\")\n",
    "    result = []\n",
    "    \n",
    "    # å¤„ç†ç©ºDataFrameæƒ…å†µ\n",
    "    if not df.empty and filter_column in df.columns:\n",
    "        back_df = df[df[filter_column] == value_for_back]\n",
    "        back_list = list(zip(back_df.get('axis-2', []).tolist(), \n",
    "                            back_df.get('axis-1', []).tolist()))[::2]\n",
    "        \n",
    "        # æŒ‰å¯¹å¤„ç†åæ ‡\n",
    "        for i in range(0, len(back_list), 2):\n",
    "            if i + 1 < len(back_list):\n",
    "                coord1 = back_list[i]\n",
    "                coord2 = back_list[i + 1]\n",
    "                \n",
    "                x_center = (coord1[0] + coord2[0]) / 2 / 256\n",
    "                y_center = (coord1[1] + coord2[1]) / 2 / 1024\n",
    "                width = (coord2[0] - coord1[0] + 40) / 2 / 256\n",
    "                height = (coord2[1] - coord1[1] + 40) / 2 / 1024  # backä½¿ç”¨40è€Œä¸æ˜¯20\n",
    "                \n",
    "                result.append((0, x_center, y_center, width, height))\n",
    "    \n",
    "    # å†™å…¥backæ–‡ä»¶ï¼ˆç¡®ä¿ç”Ÿæˆï¼‰\n",
    "    back_file_name = labels_path / (name + '_back.txt')  # ä¿ç•™åŸå§‹åç§°ä¸­çš„ç©ºæ ¼\n",
    "    print(f\"ğŸ“ æ­£åœ¨å†™å…¥backæ–‡ä»¶: {back_file_name}\")\n",
    "    with open(back_file_name, 'w', encoding='utf-8') as f:\n",
    "        for sublist in result:\n",
    "            line = ' '.join(map(str, sublist))\n",
    "            f.write(line + '\\n')\n",
    "    \n",
    "    print(f\"âœ… Backæ–‡ä»¶å·²ä¿å­˜: {back_file_name}\")\n",
    "    print(f\"   ğŸ“„ ç»å¯¹è·¯å¾„: {os.path.abspath(back_file_name)}\")\n",
    "    print(f\"   ğŸ“¦ æ–‡ä»¶å¤§å°: {os.path.getsize(back_file_name)} å­—èŠ‚\")\n",
    "    print(f\"   ğŸ“Š åŒ…å« {len(result)} ä¸ªæ ‡ç­¾\")\n",
    "\n",
    "def process_single_file(csv_path, output_dir):\n",
    "    \"\"\"å¤„ç†å•ä¸ªCSVæ–‡ä»¶ï¼Œå¢å¼ºäº†å¯¹ç©ºæ–‡ä»¶å’Œå¼‚å¸¸çš„å¤„ç†\"\"\"\n",
    "    try:\n",
    "        print(f\"\\nğŸ“– æ­£åœ¨å¤„ç†: {os.path.basename(csv_path)}\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        print(f\"   ğŸ“Š æ•°æ®ä¿¡æ¯: {len(df)} è¡Œ, {len(df.columns)} åˆ—\")\n",
    "        \n",
    "        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨ï¼ˆç©ºæ–‡ä»¶å…è®¸ç¼ºå°‘åˆ—ï¼‰\n",
    "        required_columns = ['axis-0', 'axis-1', 'axis-2']\n",
    "        if not df.empty:\n",
    "            missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "            if missing_columns:\n",
    "                print(f\"   âŒ è·³è¿‡: ç¼ºå°‘å¿…è¦çš„åˆ— {missing_columns}\")\n",
    "                return False\n",
    "        \n",
    "        # æ˜¾ç¤ºaxis-0çš„å€¼åˆ†å¸ƒ\n",
    "        if not df.empty and 'axis-0' in df.columns:\n",
    "            axis_0_counts = df['axis-0'].value_counts().sort_index()\n",
    "            print(f\"   ğŸ“ˆ axis-0 åˆ†å¸ƒ: \", end=\"\")\n",
    "            for value, count in axis_0_counts.items():\n",
    "                print(f\"{value}:{count}ä¸ª \", end=\"\")\n",
    "            print()\n",
    "        else:\n",
    "            print(\"   ğŸ“ˆ axis-0 åˆ†å¸ƒ: æ— æ•°æ®\")\n",
    "        \n",
    "        # æå–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰\n",
    "        file_name = os.path.splitext(os.path.basename(csv_path))[0]\n",
    "        \n",
    "        # è°ƒç”¨å¤„ç†å‡½æ•° - å³ä½¿æ²¡æœ‰æ•°æ®ä¹Ÿä¼šç”Ÿæˆæ–‡ä»¶\n",
    "        get_label(df, file_name, output_dir)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ å¤„ç†å¤±è´¥: {e}\")\n",
    "        # å¤„ç†å¤±è´¥æ—¶ä¹Ÿç”Ÿæˆç©ºæ–‡ä»¶\n",
    "        file_name = os.path.splitext(os.path.basename(csv_path))[0]\n",
    "        get_label(pd.DataFrame(), file_name, output_dir)\n",
    "        return True  # è§†ä¸ºæˆåŠŸï¼Œå› ä¸ºç”Ÿæˆäº†ç©ºæ–‡ä»¶\n",
    "\n",
    "def main():\n",
    "    # CSVæ–‡ä»¶å¤¹è·¯å¾„\n",
    "    csv_folder = r\"/home/gonghanmei/datasets/sz_data_all/merged_folder\"\n",
    "    \n",
    "    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹\n",
    "    output_folder = os.path.join(csv_folder, \"processed_labels\")\n",
    "    \n",
    "    print(f\"ğŸ” æ‰«ææ–‡ä»¶å¤¹: {csv_folder}\")\n",
    "    print(f\"ğŸ“ è¾“å‡ºæ–‡ä»¶å¤¹: {output_folder}\")\n",
    "    \n",
    "    # åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    print(f\"âœ… è¾“å‡ºæ–‡ä»¶å¤¹å·²åˆ›å»º: {output_folder}\")\n",
    "    \n",
    "    # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨\n",
    "    if not os.path.exists(csv_folder):\n",
    "        print(f\"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶å¤¹ {csv_folder}\")\n",
    "        print(\"è¯·æ£€æŸ¥æ–‡ä»¶å¤¹è·¯å¾„æ˜¯å¦æ­£ç¡®\")\n",
    "        return\n",
    "    \n",
    "    # æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶\n",
    "    csv_files = []\n",
    "    for file in os.listdir(csv_folder):\n",
    "        if file.lower().endswith('.csv'):\n",
    "            csv_files.append(os.path.join(csv_folder, file))\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"âŒ åœ¨æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°CSVæ–‡ä»¶\")\n",
    "        print(f\"ğŸ“ æ–‡ä»¶å¤¹å†…å®¹:\")\n",
    "        for file in os.listdir(csv_folder):\n",
    "            print(f\"   - {file}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ğŸ“‹ æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶:\")\n",
    "    for i, file in enumerate(csv_files, 1):\n",
    "        print(f\"   {i}. {os.path.basename(file)}\")\n",
    "    \n",
    "    print(f\"\\nğŸ”„ å¼€å§‹æ‰¹é‡å¤„ç†...\")\n",
    "    \n",
    "    success_count = 0\n",
    "    total_count = len(csv_files)\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        if process_single_file(csv_file, output_folder):\n",
    "            success_count += 1\n",
    "    \n",
    "    print(f\"\\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼\")\n",
    "    print(f\"ğŸ“Š ç»Ÿè®¡: {success_count}/{total_count} ä¸ªæ–‡ä»¶å¤„ç†æˆåŠŸ\")\n",
    "    \n",
    "    if success_count > 0:\n",
    "        print(f\"\\nğŸ“ æ‰€æœ‰è¾“å‡ºæ–‡ä»¶éƒ½ä¿å­˜åœ¨: {output_folder}\")\n",
    "        print(\"ç”Ÿæˆçš„æ–‡ä»¶æ ¼å¼: æ–‡ä»¶å_front.txt å’Œ æ–‡ä»¶å_back.txt\")\n",
    "        \n",
    "        # æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨\n",
    "        print(f\"\\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨:\")\n",
    "        txt_files = [f for f in os.listdir(output_folder) if f.endswith('.txt')]\n",
    "        txt_files.sort()\n",
    "        for i, txt_file in enumerate(txt_files, 1):\n",
    "            file_size = os.path.getsize(os.path.join(output_folder, txt_file))\n",
    "            print(f\"   {i}. {txt_file} ({file_size} å­—èŠ‚)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99948100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6å¯è§†åŒ–æ™®é€šå¯è§†åŒ–æ ‡ç­¾+å›¾åƒ\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def visualize_yolo_labels(image_dir, label_dir, output_dir, class_names=None):\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ–YOLOæ ¼å¼çš„æ ‡ç­¾åˆ°å¯¹åº”çš„å›¾ç‰‡ä¸Š\n",
    "    \n",
    "    å‚æ•°:\n",
    "        image_dir: å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„\n",
    "        label_dir: æ ‡ç­¾æ–‡ä»¶å¤¹è·¯å¾„\n",
    "        output_dir: è¾“å‡ºå¯è§†åŒ–ç»“æœçš„æ–‡ä»¶å¤¹è·¯å¾„\n",
    "        class_names: ç±»åˆ«åç§°åˆ—è¡¨ï¼Œå¦‚æœæ²¡æœ‰åˆ™åªæ˜¾ç¤ºç±»åˆ«ID\n",
    "    \"\"\"\n",
    "    # åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "    image_files = [f for f in os.listdir(image_dir) \n",
    "                  if os.path.splitext(f)[1].lower() in image_extensions]\n",
    "    \n",
    "    for image_file in image_files:\n",
    "        # è·å–å›¾ç‰‡è·¯å¾„å’Œæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        base_name = os.path.splitext(image_file)[0]\n",
    "        \n",
    "        # å¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶è·¯å¾„\n",
    "        label_file = os.path.join(label_dir, f\"{base_name}.txt\")\n",
    "        \n",
    "        # æ£€æŸ¥æ ‡ç­¾æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "        if not os.path.exists(label_file):\n",
    "            print(f\"è­¦å‘Š: æœªæ‰¾åˆ° {image_file} å¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶ {label_file}\")\n",
    "            continue\n",
    "        \n",
    "        # è¯»å–å›¾ç‰‡\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"è­¦å‘Š: æ— æ³•è¯»å–å›¾ç‰‡ {image_file}\")\n",
    "            continue\n",
    "        \n",
    "        height, width = image.shape[:2]\n",
    "        \n",
    "        # è¯»å–å¹¶è§£ææ ‡ç­¾æ–‡ä»¶\n",
    "        with open(label_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        # ç»˜åˆ¶æ¯ä¸ªç›®æ ‡çš„è¾¹ç•Œæ¡†\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # YOLOæ ¼å¼: class_id center_x center_y width height (å‡ä¸ºå½’ä¸€åŒ–å€¼)\n",
    "            parts = line.split()\n",
    "            if len(parts) != 5:\n",
    "                print(f\"è­¦å‘Š: æ ‡ç­¾æ–‡ä»¶ {label_file} ä¸­æ ¼å¼é”™è¯¯: {line}\")\n",
    "                continue\n",
    "            \n",
    "            class_id = int(parts[0])\n",
    "            center_x = float(parts[1])\n",
    "            center_y = float(parts[2])\n",
    "            bbox_width = float(parts[3])\n",
    "            bbox_height = float(parts[4])\n",
    "            \n",
    "            # è½¬æ¢ä¸ºç»å¯¹åæ ‡\n",
    "            x = int((center_x - bbox_width / 2) * width)\n",
    "            y = int((center_y - bbox_height / 2) * height)\n",
    "            w = int(bbox_width * width)\n",
    "            h = int(bbox_height * height)\n",
    "            \n",
    "            # ç»˜åˆ¶è¾¹ç•Œæ¡† - ä¿®æ”¹ä¸ºçº¢è‰²ä¸”çº¿æ¡æ›´ç»†\n",
    "            # color = (0, 0, 255)  # çº¢è‰² (BGRæ ¼å¼ï¼Œæ‰€ä»¥æ˜¯0,0,255)\n",
    "            color = (255, 0, 0) \n",
    "            thickness = 1  # çº¿æ¡ç²—ç»†ï¼Œæ”¹ä¸º1ä½¿è¾¹æ¡†æ›´ç»†\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color, thickness)\n",
    "            \n",
    "            # ç»˜åˆ¶ç±»åˆ«ä¿¡æ¯\n",
    "            if class_names and class_id < len(class_names):\n",
    "                label = f\"{class_names[class_id]}\"\n",
    "            else:\n",
    "                label = f\"Class {class_id}\"\n",
    "            \n",
    "            # # æ”¾ç½®æ ‡ç­¾æ–‡æœ¬ï¼ˆä½¿ç”¨çº¢è‰²ï¼‰\n",
    "            # cv2.putText(image, label, (x, y - 10), \n",
    "            #            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)  # ä¹Ÿå‡å°äº†å­—ä½“å’Œçº¿æ¡ç²—ç»†\n",
    "        \n",
    "        # ä¿å­˜å¯è§†åŒ–ç»“æœ\n",
    "        output_path = os.path.join(output_dir, image_file)\n",
    "        cv2.imwrite(output_path, image)\n",
    "        print(f\"å·²å¤„ç†: {image_file} -> ä¿å­˜åˆ° {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # é…ç½®è·¯å¾„ï¼ˆè¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰\n",
    "    IMAGE_DIR = r\"/home/gonghanmei/datasets/shen_data/all_data/png-all-jq\"   # å›¾ç‰‡æ‰€åœ¨æ–‡ä»¶å¤¹\n",
    "    LABEL_DIR = r\"/home/gonghanmei/datasets/shen_data/all_data/pred_label\"# æ ‡ç­¾æ‰€åœ¨æ–‡ä»¶å¤¹\n",
    "    # LABEL_DIR = r\"/home/gonghanmei/datasets/sz_data_all/merged_folder/processed_labels\"\n",
    "    OUTPUT_DIR = r\"/home/gonghanmei/datasets/shen_data/all_data/png-all-jq/vis\" # å¯è§†åŒ–ç»“æœè¾“å‡ºæ–‡ä»¶å¤¹\n",
    "    \n",
    "    # ç±»åˆ«åç§°åˆ—è¡¨ï¼Œå¦‚æœæœ‰çš„è¯\n",
    "    # ä¾‹å¦‚: CLASS_NAMES = [\"class1\", \"class2\", \"class3\"]\n",
    "    CLASS_NAMES = None  # å¦‚æœæ²¡æœ‰ç±»åˆ«åç§°åˆ—è¡¨ï¼Œè®¾ä¸ºNone\n",
    "    \n",
    "    # æ‰§è¡Œå¯è§†åŒ–\n",
    "    visualize_yolo_labels(IMAGE_DIR, LABEL_DIR, OUTPUT_DIR, CLASS_NAMES)\n",
    "    print(\"æ‰€æœ‰å›¾ç‰‡å¤„ç†å®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9becf98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6å¯è§†åŒ–+è™šçº¿\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def visualize_yolo_labels(image_dir, label_dir, output_dir, class_names=None, use_dashed_box=False):\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ–YOLOæ ¼å¼çš„æ ‡ç­¾åˆ°å¯¹åº”çš„å›¾ç‰‡ä¸Š\n",
    "    \n",
    "    å‚æ•°:\n",
    "        image_dir: å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„\n",
    "        label_dir: æ ‡ç­¾æ–‡ä»¶å¤¹è·¯å¾„\n",
    "        output_dir: è¾“å‡ºå¯è§†åŒ–ç»“æœçš„æ–‡ä»¶å¤¹è·¯å¾„\n",
    "        class_names: ç±»åˆ«åç§°åˆ—è¡¨ï¼Œå¦‚æœæ²¡æœ‰åˆ™åªæ˜¾ç¤ºç±»åˆ«ID\n",
    "        use_dashed_box: æ˜¯å¦ä½¿ç”¨è™šçº¿æ¡†ï¼Œé»˜è®¤ä¸ºFalseï¼ˆå®çº¿æ¡†ï¼‰\n",
    "    \"\"\"\n",
    "    # åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "    image_files = [f for f in os.listdir(image_dir) \n",
    "                  if os.path.splitext(f)[1].lower() in image_extensions]\n",
    "    \n",
    "    for image_file in image_files:\n",
    "        # è·å–å›¾ç‰‡è·¯å¾„å’Œæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        base_name = os.path.splitext(image_file)[0]\n",
    "        \n",
    "        # å¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶è·¯å¾„\n",
    "        label_file = os.path.join(label_dir, f\"{base_name}.txt\")\n",
    "        \n",
    "        # æ£€æŸ¥æ ‡ç­¾æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "        if not os.path.exists(label_file):\n",
    "            print(f\"è­¦å‘Š: æœªæ‰¾åˆ° {image_file} å¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶ {label_file}\")\n",
    "            continue\n",
    "        \n",
    "        # è¯»å–å›¾ç‰‡\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"è­¦å‘Š: æ— æ³•è¯»å–å›¾ç‰‡ {image_file}\")\n",
    "            continue\n",
    "        \n",
    "        height, width = image.shape[:2]\n",
    "        \n",
    "        # è¯»å–å¹¶è§£ææ ‡ç­¾æ–‡ä»¶\n",
    "        with open(label_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        # ç»˜åˆ¶æ¯ä¸ªç›®æ ‡çš„è¾¹ç•Œæ¡†\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # YOLOæ ¼å¼: class_id center_x center_y width height (å‡ä¸ºå½’ä¸€åŒ–å€¼)\n",
    "            parts = line.split()\n",
    "            if len(parts) != 5:\n",
    "                print(f\"è­¦å‘Š: æ ‡ç­¾æ–‡ä»¶ {label_file} ä¸­æ ¼å¼é”™è¯¯: {line}\")\n",
    "                continue\n",
    "            \n",
    "            class_id = int(parts[0])\n",
    "            center_x = float(parts[1])\n",
    "            center_y = float(parts[2])\n",
    "            bbox_width = float(parts[3])\n",
    "            bbox_height = float(parts[4])\n",
    "            \n",
    "            # è½¬æ¢ä¸ºç»å¯¹åæ ‡\n",
    "            x = int((center_x - bbox_width / 2) * width)\n",
    "            y = int((center_y - bbox_height / 2) * height)\n",
    "            w = int(bbox_width * width)\n",
    "            h = int(bbox_height * height)\n",
    "            \n",
    "            # ç»˜åˆ¶è¾¹ç•Œæ¡† - ä¿®æ”¹ä¸ºçº¢è‰²ä¸”çº¿æ¡æ›´ç»†\n",
    "            # color = (0, 0, 255)  # çº¢è‰² (BGRæ ¼å¼ï¼Œæ‰€ä»¥æ˜¯0,0,255)\n",
    "            color = (255, 0, 0) \n",
    "            # color = (0, 210, 210) \n",
    "            thickness = 1  # çº¿æ¡ç²—ç»†ï¼Œæ”¹ä¸º1ä½¿è¾¹æ¡†æ›´ç»†\n",
    "            \n",
    "            if use_dashed_box:\n",
    "                # ç»˜åˆ¶å¯†é›†å°ç‚¹çš„è™šçº¿æ¡†\n",
    "                dot_length = 1    # ç‚¹çš„é•¿åº¦\n",
    "                gap_length = 4    # ç‚¹ä¹‹é—´çš„é—´éš”\n",
    "                # ä¸Šè¾¹\n",
    "                for i in range(x, x + w, dot_length + gap_length):\n",
    "                    end_x = min(i + dot_length, x + w)\n",
    "                    cv2.line(image, (i, y), (end_x, y), color, thickness)\n",
    "                # ä¸‹è¾¹\n",
    "                for i in range(x, x + w, dot_length + gap_length):\n",
    "                    end_x = min(i + dot_length, x + w)\n",
    "                    cv2.line(image, (i, y + h), (end_x, y + h), color, thickness)\n",
    "                # å·¦è¾¹\n",
    "                for i in range(y, y + h, dot_length + gap_length):\n",
    "                    end_y = min(i + dot_length, y + h)\n",
    "                    cv2.line(image, (x, i), (x, end_y), color, thickness)\n",
    "                # å³è¾¹\n",
    "                for i in range(y, y + h, dot_length + gap_length):\n",
    "                    end_y = min(i + dot_length, y + h)\n",
    "                    cv2.line(image, (x + w, i), (x + w, end_y), color, thickness)\n",
    "            else:\n",
    "                # ç»˜åˆ¶å®çº¿æ¡†\n",
    "                cv2.rectangle(image, (x, y), (x + w, y + h), color, thickness)\n",
    "            \n",
    "            # ç»˜åˆ¶ç±»åˆ«ä¿¡æ¯\n",
    "            if class_names and class_id < len(class_names):\n",
    "                label = f\"{class_names[class_id]}\"\n",
    "            else:\n",
    "                label = f\"Class {class_id}\"\n",
    "            \n",
    "            # # æ”¾ç½®æ ‡ç­¾æ–‡æœ¬ï¼ˆä½¿ç”¨çº¢è‰²ï¼‰\n",
    "            # cv2.putText(image, label, (x, y - 10), \n",
    "            #            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)  # ä¹Ÿå‡å°äº†å­—ä½“å’Œçº¿æ¡ç²—ç»†\n",
    "        \n",
    "        # ä¿å­˜å¯è§†åŒ–ç»“æœ\n",
    "        output_path = os.path.join(output_dir, image_file)\n",
    "        cv2.imwrite(output_path, image)\n",
    "        print(f\"å·²å¤„ç†: {image_file} -> ä¿å­˜åˆ° {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # é…ç½®è·¯å¾„ï¼ˆè¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰\n",
    "    IMAGE_DIR = r\"/home/gonghanmei/project/yolo/vis_predict\"   # å›¾ç‰‡æ‰€åœ¨æ–‡ä»¶å¤¹\n",
    "    LABEL_DIR = r\"/home/gonghanmei/project/yolo/review_data_reg/labels/predict\"# æ ‡ç­¾æ‰€åœ¨æ–‡ä»¶å¤¹\n",
    "    # LABEL_DIR = r\"/home/gonghanmei/datasets/sz_data_all/merged_folder/processed_labels\"\n",
    "    OUTPUT_DIR = r\"/home/gonghanmei/project/yolo/vis_predict\" # å¯è§†åŒ–ç»“æœè¾“å‡ºæ–‡ä»¶å¤¹\n",
    "    \n",
    "\n",
    "\n",
    "    # ç±»åˆ«åç§°åˆ—è¡¨ï¼Œå¦‚æœæœ‰çš„è¯\n",
    "    # ä¾‹å¦‚: CLASS_NAMES = [\"class1\", \"class2\", \"class3\"]\n",
    "    CLASS_NAMES = None  # å¦‚æœæ²¡æœ‰ç±»åˆ«åç§°åˆ—è¡¨ï¼Œè®¾ä¸ºNone\n",
    "    \n",
    "    # æ‰§è¡Œå¯è§†åŒ–ï¼Œè®¾ç½®use_dashed_box=Trueä½¿ç”¨å¯†é›†å°ç‚¹è™šçº¿æ¡†ï¼ŒFalseä½¿ç”¨å®çº¿æ¡†\n",
    "    visualize_yolo_labels(IMAGE_DIR, LABEL_DIR, OUTPUT_DIR, CLASS_NAMES, use_dashed_box=True)\n",
    "    print(\"æ‰€æœ‰å›¾ç‰‡å¤„ç†å®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b4ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è‡ªé€‚åº”è£å‰ª\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "def find_bottom_content(img_array, threshold=10):\n",
    "    \"\"\"\n",
    "    æ‰¾åˆ°å›¾åƒåº•éƒ¨æœ‰å†…å®¹çš„æœ€åä¸€è¡Œ\n",
    "    \n",
    "    å‚æ•°:\n",
    "        img_array: å›¾åƒçš„numpyæ•°ç»„\n",
    "        threshold: åƒç´ å€¼é˜ˆå€¼ï¼Œå¤§äºæ­¤å€¼è®¤ä¸ºæ˜¯å†…å®¹\n",
    "    \n",
    "    è¿”å›:\n",
    "        æœ€åä¸€è¡Œæœ‰å†…å®¹çš„è¡Œç´¢å¼•\n",
    "    \"\"\"\n",
    "    height = img_array.shape[0]\n",
    "    \n",
    "    # ä»åº•éƒ¨å‘ä¸Šæ‰«æ\n",
    "    for i in range(height - 1, -1, -1):\n",
    "        row = img_array[i]\n",
    "        # å¦‚æœè¿™ä¸€è¡Œæœ‰ä»»ä½•åƒç´ å€¼å¤§äºé˜ˆå€¼ï¼Œè®¤ä¸ºæ˜¯æœ‰å†…å®¹çš„\n",
    "        if np.any(row > threshold):\n",
    "            return i\n",
    "    \n",
    "    return height - 1\n",
    "\n",
    "def crop_bottom_black(input_path, output_path, bottom_margin=20, threshold=10):\n",
    "    \"\"\"\n",
    "    è£å‰ªå›¾åƒåº•éƒ¨çš„é»‘è‰²åŒºåŸŸï¼Œä¿ç•™æŒ‡å®šçš„åº•éƒ¨è¾¹è·\n",
    "    \n",
    "    å‚æ•°:\n",
    "        input_path: è¾“å…¥å›¾åƒè·¯å¾„\n",
    "        output_path: è¾“å‡ºå›¾åƒè·¯å¾„\n",
    "        bottom_margin: åº•éƒ¨ä¿ç•™çš„åƒç´ æ•°\n",
    "        threshold: åˆ¤æ–­æ˜¯å¦ä¸ºé»‘è‰²çš„é˜ˆå€¼\n",
    "    \"\"\"\n",
    "    # è¯»å–å›¾åƒ\n",
    "    img = Image.open(input_path)\n",
    "    \n",
    "    # è½¬æ¢ä¸ºnumpyæ•°ç»„\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # å¤„ç†ä¸åŒçš„å›¾åƒæ ¼å¼\n",
    "    if len(img_array.shape) == 3:  # RGBæˆ–RGBA\n",
    "        # ä½¿ç”¨ç°åº¦å€¼åˆ¤æ–­\n",
    "        gray = np.mean(img_array[:, :, :3], axis=2)\n",
    "    else:  # ç°åº¦å›¾\n",
    "        gray = img_array\n",
    "    \n",
    "    # æ‰¾åˆ°åº•éƒ¨æœ€åæœ‰å†…å®¹çš„è¡Œ\n",
    "    last_content_row = find_bottom_content(gray, threshold)\n",
    "    \n",
    "    # è®¡ç®—è£å‰ªä½ç½®ï¼ˆä¿ç•™bottom_marginåƒç´ ï¼‰\n",
    "    crop_bottom = min(last_content_row + bottom_margin, img_array.shape[0])\n",
    "    \n",
    "    # è£å‰ªå›¾åƒ\n",
    "    cropped_array = img_array[:crop_bottom, :]\n",
    "    \n",
    "    # è½¬æ¢å›PILå›¾åƒå¹¶ä¿å­˜\n",
    "    cropped_img = Image.fromarray(cropped_array)\n",
    "    cropped_img.save(output_path)\n",
    "    \n",
    "    return crop_bottom\n",
    "\n",
    "def batch_crop_images(input_folder, output_folder=None, bottom_margin=20, threshold=10):\n",
    "    \"\"\"\n",
    "    æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰PNGå›¾åƒ\n",
    "    \n",
    "    å‚æ•°:\n",
    "        input_folder: è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„\n",
    "        output_folder: è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™åœ¨è¾“å…¥æ–‡ä»¶å¤¹åˆ›å»ºcroppedå­æ–‡ä»¶å¤¹ï¼‰\n",
    "        bottom_margin: åº•éƒ¨ä¿ç•™çš„åƒç´ æ•°\n",
    "        threshold: åˆ¤æ–­æ˜¯å¦ä¸ºé»‘è‰²çš„é˜ˆå€¼\n",
    "    \"\"\"\n",
    "    input_path = Path(input_folder)\n",
    "    \n",
    "    # è®¾ç½®è¾“å‡ºæ–‡ä»¶å¤¹\n",
    "    if output_folder is None:\n",
    "        output_path = input_path / \"cropped\"\n",
    "    else:\n",
    "        output_path = Path(output_folder)\n",
    "    \n",
    "    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # è·å–æ‰€æœ‰PNGæ–‡ä»¶\n",
    "    png_files = list(input_path.glob(\"*.png\")) + list(input_path.glob(\"*.PNG\"))\n",
    "    \n",
    "    if not png_files:\n",
    "        print(f\"åœ¨ {input_folder} ä¸­æ²¡æœ‰æ‰¾åˆ°PNGæ–‡ä»¶\")\n",
    "        return\n",
    "    \n",
    "    print(f\"æ‰¾åˆ° {len(png_files)} ä¸ªPNGæ–‡ä»¶\")\n",
    "    print(f\"å¼€å§‹å¤„ç†ï¼Œåº•éƒ¨ä¿ç•™ {bottom_margin} åƒç´ ...\")\n",
    "    \n",
    "    # å¤„ç†æ¯ä¸ªæ–‡ä»¶\n",
    "    for i, png_file in enumerate(png_files, 1):\n",
    "        try:\n",
    "            output_file = output_path / png_file.name\n",
    "            crop_height = crop_bottom_black(\n",
    "                str(png_file), \n",
    "                str(output_file), \n",
    "                bottom_margin=bottom_margin,\n",
    "                threshold=threshold\n",
    "            )\n",
    "            print(f\"[{i}/{len(png_files)}] {png_file.name} - è£å‰ªè‡³é«˜åº¦ {crop_height}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{i}/{len(png_files)}] å¤„ç† {png_file.name} æ—¶å‡ºé”™: {e}\")\n",
    "    \n",
    "    print(f\"\\nå®Œæˆ! å¤„ç†åçš„å›¾åƒä¿å­˜åœ¨: {output_path}\")\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "if __name__ == \"__main__\":\n",
    "    # è®¾ç½®ä½ çš„æ–‡ä»¶å¤¹è·¯å¾„\n",
    "    input_folder = \"/home/gonghanmei/project/yolo/vis_predict\"\n",
    "    \n",
    "    # æ–¹å¼1: è‡ªåŠ¨åˆ›å»ºcroppedå­æ–‡ä»¶å¤¹\n",
    "    batch_crop_images(input_folder, bottom_margin=20)\n",
    "    \n",
    "    # æ–¹å¼2: æŒ‡å®šè¾“å‡ºæ–‡ä»¶å¤¹\n",
    "    # output_folder = \"/home/gonghanmei/datasets/shen_data/png-all-jq/vis_cropped\"\n",
    "    # batch_crop_images(input_folder, output_folder, bottom_margin=20)\n",
    "    \n",
    "    # å¦‚æœéœ€è¦è°ƒæ•´é˜ˆå€¼ï¼ˆé»˜è®¤10ï¼‰ï¼Œå¯ä»¥è¿™æ ·ï¼š\n",
    "    batch_crop_images(input_folder, bottom_margin=20, threshold=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50f2284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æå–åˆ†å‰²å¯è§†åŒ–\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "def collect_compare_images(search_root='seg_output_full5', collection_dir='compare_images_collection6'):\n",
    "    \"\"\"\n",
    "    é€’å½’æŸ¥æ‰¾æŒ‡å®šç›®å½•ä¸‹æ‰€æœ‰ä»¥ '_compare.png' ç»“å°¾çš„æ–‡ä»¶ï¼Œå¹¶å°†å®ƒä»¬å¤åˆ¶åˆ°ç›®æ ‡é›†åˆç›®å½•ã€‚\n",
    "\n",
    "    Args:\n",
    "        search_root (str): å¼€å§‹æœç´¢çš„æ ¹ç›®å½•ã€‚\n",
    "        collection_dir (str): å¤åˆ¶æ–‡ä»¶çš„ç›®æ ‡ç›®å½•ã€‚\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"--- å›¾åƒæå–ä»»åŠ¡å¯åŠ¨ ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')}) ---\")\n",
    "    print(f\"ğŸ” æœç´¢ç›®å½•: {os.path.abspath(search_root)}\")\n",
    "    \n",
    "    # 1. ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨\n",
    "    try:\n",
    "        os.makedirs(collection_dir, exist_ok=True)\n",
    "        print(f\"ğŸ“‚ ç›®æ ‡ç›®å½•å·²å‡†å¤‡: {os.path.abspath(collection_dir)}\")\n",
    "    except OSError as e:\n",
    "        print(f\"âŒ é”™è¯¯ï¼šæ— æ³•åˆ›å»ºç›®æ ‡ç›®å½• {collection_dir}ã€‚æƒé™æˆ–è·¯å¾„é—®é¢˜ã€‚\")\n",
    "        print(f\"é”™è¯¯ä¿¡æ¯: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. å®šä¹‰æœç´¢æ¨¡å¼å¹¶æ‰§è¡Œé€’å½’æŸ¥æ‰¾\n",
    "    # '**/*' è¡¨ç¤ºé€’å½’åœ°è¿›å…¥æ‰€æœ‰å­ç›®å½•\n",
    "    file_pattern = os.path.join(search_root, '**/*_compare.png')\n",
    "    \n",
    "    # ä½¿ç”¨ glob.glob å’Œ recursive=True è¿›è¡ŒæŸ¥æ‰¾\n",
    "    compare_files = glob.glob(file_pattern, recursive=True)\n",
    "\n",
    "    if not compare_files:\n",
    "        print(\"\\nâš ï¸ æœªæ‰¾åˆ°ä»»ä½•åŒ¹é… '_compare.png' åç¼€çš„æ–‡ä»¶ã€‚è¯·æ£€æŸ¥è·¯å¾„å’Œæ–‡ä»¶åã€‚\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nâœ… æ‰¾åˆ° {len(compare_files)} ä¸ªåŒ¹é…æ–‡ä»¶ï¼Œå¼€å§‹å¤åˆ¶...\")\n",
    "    \n",
    "    copied_count = 0\n",
    "    \n",
    "    # 3. éå†å¹¶å¤åˆ¶æ–‡ä»¶\n",
    "    for src_path in compare_files:\n",
    "        # è·å–æ–‡ä»¶å (ä¾‹å¦‚: 2024-10-2508-PengMeiYun_front_compare.png)\n",
    "        file_name = os.path.basename(src_path)\n",
    "        \n",
    "        # æ„é€ ç›®æ ‡æ–‡ä»¶çš„å®Œæ•´è·¯å¾„\n",
    "        dst_path = os.path.join(collection_dir, file_name)\n",
    "        \n",
    "        # ä¸ºäº†é¿å…æ–‡ä»¶åå†²çªï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ–‡ä»¶åä¸­æ·»åŠ ä¸€ä¸ªå”¯ä¸€çš„IDæˆ–ç›®å½•å\n",
    "        # æ¯”å¦‚ï¼šå°†å…¶çˆ¶ç›®å½•åä½œä¸ºå‰ç¼€ï¼ˆä¾‹å¦‚ï¼š2024-10-2508-PengMeiYun_front_compare.png -> PengMeiYun_front_2024-10-2508-PengMeiYun_front_compare.pngï¼‰\n",
    "        # é‰´äºæ‚¨çš„è¾“å…¥è·¯å¾„ç»“æ„ï¼Œæ–‡ä»¶é‡åå¯èƒ½æ€§è¾ƒå¤§ï¼Œæˆ‘ä»¬é‡‡ç”¨æ›´ç¨³å¥çš„é‡å‘½åç­–ç•¥ï¼š\n",
    "        if os.path.exists(dst_path):\n",
    "            # è·å–ä¸Šçº§ç›®å½•åï¼Œä¾‹å¦‚ '2024-10-2508-PengMeiYun_front'\n",
    "            parent_dir_name = os.path.basename(os.path.dirname(src_path))\n",
    "            \n",
    "            # åˆ›å»ºæ–°çš„æ–‡ä»¶åï¼šçˆ¶ç›®å½•å_æ–‡ä»¶å\n",
    "            new_file_name = f\"{parent_dir_name}_{file_name}\"\n",
    "            dst_path = os.path.join(collection_dir, new_file_name)\n",
    "            \n",
    "            print(f\"   * é‡å‘½åï¼š{file_name} -> {new_file_name}\")\n",
    "\n",
    "        try:\n",
    "            # ä½¿ç”¨ shutil.copy2 å¤åˆ¶æ–‡ä»¶ï¼Œå®ƒä¼šä¿ç•™æ–‡ä»¶çš„å…ƒæ•°æ®ï¼ˆå¦‚åˆ›å»ºæ—¶é—´ï¼‰\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "            # print(f\"   - å¤åˆ¶æˆåŠŸï¼š{src_path}\") # è°ƒè¯•ä¿¡æ¯ï¼Œå¯ä»¥å–æ¶ˆæ³¨é‡Š\n",
    "            copied_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ å¤åˆ¶å¤±è´¥ï¼š{src_path} åˆ° {dst_path}\")\n",
    "            print(f\"   é”™è¯¯ä¿¡æ¯: {e}\")\n",
    "\n",
    "    print(f\"\\n--- ä»»åŠ¡å®Œæˆ ---\")\n",
    "    print(f\"âœ¨ æˆåŠŸå¤åˆ¶ {copied_count} ä¸ªæ–‡ä»¶åˆ° {collection_dir} ç›®å½•ã€‚\")\n",
    "\n",
    "\n",
    "# è¿è¡Œå‡½æ•°\n",
    "if __name__ == \"__main__\":\n",
    "    # è¯·ç¡®ä¿æ‚¨åœ¨è¿è¡Œè„šæœ¬æ—¶ï¼Œ'seg_output_full' ç›®å½•ä½äºå½“å‰å·¥ä½œç›®å½•ä¸‹ï¼Œ\n",
    "    # æˆ–è€…å°† search_root è®¾ç½®ä¸ºç»å¯¹è·¯å¾„ã€‚\n",
    "    collect_compare_images()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
